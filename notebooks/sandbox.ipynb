{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from io import StringIO\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path(\"D:/code/frankie_edgar_stan_zone\") / \"data\"\n",
    "FIGHT_LINKS_PICKLE = BASE_PATH / \"fight_links.pickle\"\n",
    "PAST_EVENT_LINKS_PICKLE = BASE_PATH / \"past_event_links.pickle\"\n",
    "PAST_FIGHTER_LINKS_PICKLE = BASE_PATH / \"past_fighter_links.pickle\"\n",
    "SCRAPED_FIGHTER_DATA_DICT_PICKLE = BASE_PATH / \"scraped_fighter_data_dict.pickle\"\n",
    "NEW_FIGHTS_DATA_PATH = BASE_PATH / \"new_fight_data.csv\"\n",
    "TOTAL_FIGHTS_DATA_PATH = BASE_PATH / \"raw_total_fight_data.csv\"\n",
    "PREPROCESSED_DATA_PATH = BASE_PATH / \"preprocessed_data.csv\"\n",
    "FIGHTER_DETAILS_DATA_PATH = BASE_PATH / \"raw_fighter_details.csv\"\n",
    "UFC_DATA_PATH = BASE_PATH / \"data.csv\"\n",
    "EVENT_DATA_PATH = BASE_PATH / \"event_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for data schema and column names go here.\n",
    "# Not needed if inferring column names from headers\n",
    "\n",
    "# some ufc table headers have wrong names in source code (TD is often labelled TD% internally)\n",
    "# which busts pd.read_html because it sees two TD% columns.\n",
    "# praying for consistent schema and using these labels instead trying\n",
    "# to parse headers will be faster and simpler than writing parsing code\n",
    "# ...if it works\n",
    "# _________\n",
    "# FIGHT DETAILS columns as listed on UFC stats website\n",
    "# e.g. http://ufcstats.com/fight-details/eaa885cf7ae31e0b\n",
    "web_fight_cols = [\n",
    "    \"FIGHTER\",\n",
    "    \"KD\",\n",
    "    \"SIG STR\",\n",
    "    \"SIG STR%\",\n",
    "    \"TOTAL STR\",\n",
    "    \"TD\",\n",
    "    \"TD%\",\n",
    "    \"REV\",\n",
    "    \"CTRL\",\n",
    "]\n",
    "\n",
    "web_strike_cols = [\n",
    "    \"FIGHTER\",\n",
    "    \"SIG STR\",\n",
    "    \"SIG STR%\",\n",
    "    \"HEAD\",\n",
    "    \"BODY\",\n",
    "    \"LEG\",\n",
    "    \"DISTANCE\",\n",
    "    \"CLINCH\",\n",
    "    \"GROUND\",\n",
    "]\n",
    "# _________\n",
    "\n",
    "# cols of event data saved locally\n",
    "event_cols = [\n",
    "    \"ID\",\n",
    "    \"TITLE\",\n",
    "    \"DATE\",\n",
    "    \"LOCATION\",\n",
    "    \"LINK\",\n",
    "    \"FIGHT_LINKS_SCRAPED\",\n",
    "    \"FIGHT_DATA_SCRAPED\",\n",
    "]\n",
    "\n",
    "# column labels for processed fight data\n",
    "# each fighter (R: red, B: blue)\n",
    "# gets total stats (_TOT) suffix\n",
    "# and round stats (_R#)\n",
    "# everyone gets stats for 5 rounds to keep schema consistant\n",
    "# doing this programatically to write less\n",
    "# these will have dependencies (tots = sum of  rounds, percents, etc)\n",
    "# that you might want to remove before shoving into an ML model\n",
    "# but making columns for everything for granularity/readability\n",
    "\n",
    "shared_cols = [\n",
    "    \"FIGHT_ID\",\n",
    "    \"FIGHT_LINK\",\n",
    "    \"TITLE_FIGHT\",  # true/false\n",
    "    \"R_FIGHTER\",\n",
    "    \"R_FIGHTER_ID\",\n",
    "    \"L_FIGHTER\",\n",
    "    \"L_FIGHTER_ID\",\n",
    "    \"WINNER\",\n",
    "    \"METHOD\",\n",
    "    \"WIN_RND\",\n",
    "    \"WIN_TIME\",\n",
    "    \"FORMAT\",  # 5 round or 3 round\n",
    "    \"DETAILS\",  # could be judge scores or more details on finish, needs processing\n",
    "    \"REFEREE\",\n",
    "    \"EVENT_ID\",\n",
    "    \"EVENT_TITLE\",\n",
    "    \"EVENT_DATE\",\n",
    "    \"EVENT_LOC\",\n",
    "    \"EVENT_BOUT_NUM\",  # 1= headliner, 2= coheadliner, etc...]\n",
    "]\n",
    "# columns without percents\n",
    "# aka stuff that doesn't need ATT, LND, and PCT suffixes\n",
    "# these could have a better variable name\n",
    "gen_stat_cols = [\"KD\", \"SUB_ATT\", \"REV\", \"CTRL_TIME\"]\n",
    "\n",
    "# head/body/leg/distance/clinch/ground\n",
    "# numbers are only for sig strikes --\n",
    "# breakdowns not included for non-sig strikes\n",
    "# omitting sig_str prefix for these for readability\n",
    "\n",
    "pct_stat_cols = [\n",
    "    \"SIG_STR\",\n",
    "    \"ALL_STR\",\n",
    "    \"TD\",\n",
    "    \"HEAD_STR\",\n",
    "    \"BODY_STR\",\n",
    "    \"LEG_STR\",\n",
    "    \"DISTANCE_STR\",\n",
    "    \"CLINCH_STR\",\n",
    "    \"GROUND_STR\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url: str) -> BeautifulSoup:\n",
    "    source_code = requests.get(url, allow_redirects=False)\n",
    "    plain_text = source_code.text.encode(\"ascii\", \"replace\")\n",
    "    return BeautifulSoup(plain_text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for adding prefixes/suffixes to dictionary labels.\n",
    "# lots of stats can be scraped via same routine iterated over some html object\n",
    "# but need different prefixes/suffixes (e.g. R or B for red or blue fighter, or R1-R5 for round)\n",
    "def add_prefix_label(old_dict: dict[str, str], prefix: str) -> dict[str, str]:\n",
    "    new_dict = {}\n",
    "\n",
    "    for lbl, val in old_dict.items():\n",
    "        new_lbl = f\"{prefix}_{lbl}\"\n",
    "        new_dict[new_lbl] = val\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "def add_suffix_label(old_dict: dict[str, str], suffix: str) -> dict[str, str]:\n",
    "    new_dict = {}\n",
    "\n",
    "    for lbl, val in old_dict.items():\n",
    "        new_lbl = f\"{lbl}_{suffix}\"\n",
    "        new_dict[new_lbl] = val\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(\n",
    "    iteration: int,\n",
    "    total: int,\n",
    "    prefix: str = \"\",\n",
    "    suffix: str = \"\",\n",
    "    decimals: int = 1,\n",
    "    bar_length: int = 50,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        bar_length  - Optional  : character length of bar (Int)\n",
    "    \"\"\"\n",
    "    percents = f\"{100 * (iteration / float(total)):.2f}\"\n",
    "    filled_length = int(round(bar_length * iteration / float(total)))\n",
    "    bar = f'{\"â–ˆ\" * filled_length}{\"-\" * (bar_length - filled_length)}'\n",
    "\n",
    "    sys.stdout.write(f\"\\r{prefix} |{bar}| {percents}% {suffix}\")\n",
    "\n",
    "    if iteration == total:\n",
    "        sys.stdout.write(\"\\n\")\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UFCLinks:\n",
    "    def __init__(\n",
    "        self, all_events_url=\"http://ufcstats.com/statistics/events/completed?page=all\"\n",
    "    ):\n",
    "        self.all_events_url = all_events_url\n",
    "        self.EVENT_DATA_PATH = EVENT_DATA_PATH\n",
    "        self.EVENT_DATA = None\n",
    "        self.FIGHT_LINKS_PICKLE_PATH = FIGHT_LINKS_PICKLE\n",
    "        self.FIGHT_LINKS = None\n",
    "        self._initiate_class()\n",
    "\n",
    "    def _scrape_all_events(self) -> pd.DataFrame:\n",
    "        # reads all events from all_events_url column and\n",
    "        # initiates event data table as dataframe.\n",
    "        event_text = \";\".join(event_cols)\n",
    "        soup = make_soup(self.all_events_url)\n",
    "        for row in soup.tbody.findAll(\"tr\", {\"class\": \"b-statistics__table-row\"}):\n",
    "\n",
    "            # case handling for blank row that exists at top of table.\n",
    "            # text is just empty string/newline chars\n",
    "            if row.text.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            link_elt = row.find(\"a\")\n",
    "            event_title = link_elt.text.strip().upper()\n",
    "            event_link = link_elt.get(\"href\")\n",
    "            event_id = event_link.split(\"/\")[-1]\n",
    "\n",
    "            event_date = (\n",
    "                row.find(\"span\", {\"class\": \"b-statistics__date\"}).text.strip().upper()\n",
    "            )\n",
    "\n",
    "            # taking for granted that event location is last td element in row.\n",
    "            event_location = row.findAll(\"td\")[-1].text.strip().upper()\n",
    "\n",
    "            event_text += \"\\n\" + \";\".join(\n",
    "                [\n",
    "                    event_id,\n",
    "                    event_title,\n",
    "                    event_date,\n",
    "                    event_location,\n",
    "                    event_link,\n",
    "                    \"False\",\n",
    "                    \"False\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # pass through stringIO so this csv like text string can be plugged into pandas read_csv\n",
    "        event_data = StringIO(event_text)\n",
    "        event_df = pd.read_csv(event_data, sep=\";\")\n",
    "        # reformat datetimes\n",
    "        event_df[\"DATE\"] = pd.to_datetime(event_df[\"DATE\"], format=\"%B %d, %Y\")\n",
    "        # change ID to index\n",
    "        event_df = event_df.set_index(\"ID\")\n",
    "\n",
    "        return event_df\n",
    "\n",
    "    def _write_event_data(self, df):\n",
    "        filepath = self.EVENT_DATA_PATH\n",
    "        df.to_csv(filepath, sep=\";\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _initiate_class(self):\n",
    "        # get latest event data from web\n",
    "        print(f\"Pulling event data from {self.all_events_url}\")\n",
    "        web_event_df = self._scrape_all_events()\n",
    "        web_event_ids = web_event_df.index\n",
    "\n",
    "        if not self.EVENT_DATA_PATH.exists():\n",
    "            # if no event data file, initate event data by writing this to csv\n",
    "            # with no comparisons\n",
    "\n",
    "            print(\n",
    "                f\"No existing event data, writing web data locally to {self.EVENT_DATA_PATH}\"\n",
    "            )\n",
    "            self._write_event_data(web_event_df)\n",
    "            # label for return data\n",
    "            event_df = web_event_df\n",
    "        else:\n",
    "            # otherwise, event data file already exists.\n",
    "            # compare with all_event_df by id and only write rows\n",
    "            # that aren't present in existing file\n",
    "            print(f\"Reading local event data from {self.EVENT_DATA_PATH}\")\n",
    "            local_event_df = pd.read_csv(\n",
    "                self.EVENT_DATA_PATH, sep=\";\", parse_dates=[\"DATE\"], index_col=\"ID\"\n",
    "            )\n",
    "\n",
    "            local_event_ids = local_event_df.index\n",
    "            new_event_ids = web_event_ids.difference(local_event_ids)\n",
    "\n",
    "            # return local data unless new events present in web data.\n",
    "\n",
    "            if not new_event_ids.empty:\n",
    "                # append  new events to beginning of DF and overwrite file\n",
    "                # we could make it only write the new rows, but this file is small enough that i don't care\n",
    "                # and sorting semantics are easier like this.\n",
    "                print(f\"{len(new_event_ids)} new event/s. Updating local event data.\")\n",
    "                #return  web_event_df, new_event_ids, local_event_df\n",
    "                updated_df = pd.concat([web_event_df.loc[new_event_ids], local_event_df])\n",
    "                self._write_event_data(updated_df)\n",
    "                # return updated event df if new events present in web\n",
    "                event_df = updated_df\n",
    "            else:\n",
    "                #otherwise, no new events, local event data still valid.\n",
    "                print(\"No new events, local data up to date\")\n",
    "                event_df = local_event_df\n",
    "\n",
    "        # set event data property\n",
    "        self.EVENT_DATA = event_df\n",
    "\n",
    "        # load fight links if they already exist.\n",
    "        if self.FIGHT_LINKS_PICKLE_PATH.exists():\n",
    "            print(f\"Loading local fight links from {self.FIGHT_LINKS_PICKLE_PATH}\")\n",
    "            # load prev events and links\n",
    "            with open(self.FIGHT_LINKS_PICKLE_PATH, \"rb\") as event_fight_dict:\n",
    "                prev_fight_links = pickle.load(event_fight_dict)\n",
    "                self.FIGHT_LINKS = prev_fight_links\n",
    "\n",
    "        return event_df\n",
    "\n",
    "    # given list of event links, gets all links to fights for that event and\n",
    "    # stores in dictionary using event link as key\n",
    "    def _make_link_dict(self, event_links: list[str]) -> dict[str, str]:\n",
    "\n",
    "        num_events = len(event_links)\n",
    "        event_fight_dict = {}\n",
    "        print(f\"Scraping fight links from {num_events} events: \")\n",
    "        print_progress(0, num_events, prefix=\"Progress:\", suffix=\"Complete\")\n",
    "        for index, link in enumerate(event_links):\n",
    "            event_fights = []\n",
    "            soup = make_soup(link)\n",
    "            for row in soup.findAll(\n",
    "                \"tr\",\n",
    "                {\n",
    "                    \"class\": \"b-fight-details__table-row b-fight-details__table-row__hover js-fight-details-click\"\n",
    "                },\n",
    "            ):\n",
    "                href = row.get(\"data-link\")\n",
    "                event_fights.append(href)\n",
    "\n",
    "            event_fight_dict[link] = event_fights\n",
    "\n",
    "            print_progress(index + 1, num_events, prefix=\"Progress:\", suffix=\"Complete\")\n",
    "\n",
    "        return event_fight_dict\n",
    "\n",
    "    def _write_fight_links(self):\n",
    "        # might not need this as a subfunction but i don't want to write it twice\n",
    "        with open(self.FIGHT_LINKS_PICKLE_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.FIGHT_LINKS, f)\n",
    "\n",
    "    def _initiate_fight_links(self):\n",
    "        # to initiate, make dict from all event data links\n",
    "        event_df = self.EVENT_DATA\n",
    "        event_fight_link_dict = self._make_link_dict(event_df[\"LINK\"])\n",
    "        return event_fight_link_dict\n",
    "\n",
    "    def _get_unscraped_fight_links(self):\n",
    "        event_df = self.EVENT_DATA\n",
    "        links_to_scrape = event_df[~event_df[\"FIGHT_LINKS_SCRAPED\"]][\"LINK\"]\n",
    "        if links_to_scrape.empty:\n",
    "            print(\"No new event links to scrape.\")\n",
    "            new_fight_links = {}\n",
    "        else:\n",
    "            new_fight_links = self._make_link_dict(links_to_scrape)\n",
    "        return new_fight_links\n",
    "\n",
    "    def _update_event_fight_link_scraped_status(self):\n",
    "        # get ids from event-fight dict keys.\n",
    "        # assuming that if event is in there, fight links have been scraped.\n",
    "        # it's not really airtight logic, but good enough for now\n",
    "        scraped_ids = [id.split(\"/\")[-1] for id in self.FIGHT_LINKS.keys()]\n",
    "        event_df = self.EVENT_DATA\n",
    "        event_df.loc[scraped_ids, \"FIGHT_LINKS_SCRAPED\"] = True\n",
    "        self.EVENT_DATA = event_df\n",
    "        self._write_event_data(event_df)\n",
    "        return event_df\n",
    "\n",
    "    def get_fight_links(self, force_refresh=False):\n",
    "\n",
    "        # if force_refresh is True, retrieves all fight links from events regardless\n",
    "        # of FIGHT_LINKS_SCRAPED value (refresh also forced if fight link file doesnt exist)\n",
    "        # otherwise, only scrapes links where FIGHT_LINKS_SCRAPED == False\n",
    "        if force_refresh or not self.FIGHT_LINKS_PICKLE_PATH.exists():\n",
    "            print(f\"Scraping all fight links to {self.FIGHT_LINKS_PICKLE_PATH}\")\n",
    "            fight_link_dict = self._initiate_fight_links()\n",
    "        else:\n",
    "            print(\"Checking for new events to scrape\")\n",
    "            new_fight_links = self._get_unscraped_fight_links()\n",
    "            fight_link_dict = self.FIGHT_LINKS.copy()\n",
    "            fight_link_dict.update(new_fight_links)\n",
    "\n",
    "        self.FIGHT_LINKS = fight_link_dict\n",
    "        self._update_event_fight_link_scraped_status()\n",
    "        self._write_fight_links()\n",
    "\n",
    "        return fight_link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling event data from http://ufcstats.com/statistics/events/completed?page=all\n",
      "Reading local event data from D:\\code\\frankie_edgar_stan_zone\\data\\event_data.csv\n",
      "4 new event/s. Updating local event data.\n",
      "Loading local fight links from D:\\code\\frankie_edgar_stan_zone\\data\\fight_links.pickle\n"
     ]
    }
   ],
   "source": [
    "ufc_links=UFCLinks()\n",
    "ufc_links.get_fight_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure if i'm actually using this\n",
    "\n",
    "def append_col_prefix_suffix(\n",
    "    fighter_prefix=(\"B\", \"R\"),\n",
    "    pct_suffix=(\"LND\", \"ATT\", \"PCT\"),\n",
    "    rnd_suffix=(\"TOT\", \"R1\", \"R2\", \"R3\", \"R4\", \"R5\"),\n",
    "    shared_cols=shared_cols,\n",
    "    gen_cols=gen_stat_cols,\n",
    "    pct_cols=pct_stat_cols,\n",
    "):\n",
    "    gen_stat_cols = [f\"{pre}_{stat}_{r_suf}\" for pre in fighter_prefix for stat in gen_cols for r_suf in rnd_suffix]\n",
    "    pct_stat_cols = [\n",
    "        f\"{pre}_{stat}_{p_suf}_{r_suf}\"\n",
    "        for pre in fighter_prefix\n",
    "        for stat in pct_cols\n",
    "        for r_suf in rnd_suffix\n",
    "        for p_suf in pct_suffix\n",
    "    ]\n",
    "    fight_cols = shared_cols + gen_stat_cols + pct_stat_cols\n",
    "    return fight_cols\n",
    "\n",
    "\n",
    "fight_cols = append_col_prefix_suffix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code for reference\n",
    "\n",
    "# from src.createdata.scrape_fight_links import UFCLinks\n",
    "# from src.createdata.utils import make_soup, print_progress\n",
    "\n",
    "# from src.createdata.data_files_path import (  # isort:skip\n",
    "#     NEW_EVENT_AND_FIGHTS,\n",
    "#     TOTAL_EVENT_AND_FIGHTS,\n",
    "# )\n",
    "\n",
    "\n",
    "# assuming red corner is always listed first.\n",
    "class FightDataScraper:\n",
    "    def __init__(self):\n",
    "        self.HEADER: str = (\n",
    "            \"R_fighter;B_fighter;R_KD;B_KD;R_SIG_STR.;B_SIG_STR.\\\n",
    ";R_SIG_STR_pct;B_SIG_STR_pct;R_TOTAL_STR.;B_TOTAL_STR.;R_TD;B_TD;R_TD_pct\\\n",
    ";B_TD_pct;R_SUB_ATT;B_SUB_ATT;R_REV;B_REV;R_CTRL;B_CTRL;R_HEAD;B_HEAD;R_BODY\\\n",
    ";B_BODY;R_LEG;B_LEG;R_DISTANCE;B_DISTANCE;R_CLINCH;B_CLINCH;R_GROUND;B_GROUND\\\n",
    ";win_by;last_round;last_round_time;Format;Referee;date;location;Fight_type;Winner\\n\"\n",
    "        )\n",
    "\n",
    "        self.NEW_FIGHTS_DATA_PATH = NEW_FIGHTS_DATA_PATH\n",
    "        self.TOTAL_FIGHTS_DATA_PATH = TOTAL_FIGHTS_DATA_PATH\n",
    "\n",
    "    def create_fight_data_csv(self) -> None:\n",
    "        print(\"Scraping links!\")\n",
    "\n",
    "        ufc_links = UFCLinks()\n",
    "        new_fight_links, all_fight_links = (\n",
    "            ufc_links.get_fight_links()\n",
    "        )\n",
    "        print(\"Successfully scraped and saved fight links!\\n\")\n",
    "        print(\"Now, scraping fight data!\\n\")\n",
    "\n",
    "        # are there new fight links to scrap data from?\n",
    "        if not new_fight_links:\n",
    "            # if there's no new fight links\n",
    "            if self.TOTAL_FIGHTS_DATA_PATH.exists():\n",
    "                # if fight data csv file exists.\n",
    "\n",
    "                # assume fight data up to date\n",
    "                # this is not actually necessarily true\n",
    "                # but good enough for now\n",
    "                print(\n",
    "                    f\"\"\"No new fight data to scrape.\n",
    "                        {self.TOTAL_EVENT_AND_FIGHTS_PATH} up to date.\"\"\"\n",
    "                )\n",
    "                return None\n",
    "            else:\n",
    "                # if no data csv, scrape all fights and make it.\n",
    "                self._scrape_raw_fight_data(\n",
    "                    all_fight_links,\n",
    "                    filepath=self.TOTAL_FIGHTS_PATH,\n",
    "                )\n",
    "        else:\n",
    "            # scrape only fights from new events\n",
    "            self._scrape_raw_fight_data(\n",
    "                new_fight_links, filepath=self.NEW_EVENT_AND_FIGHTS_PATH\n",
    "            )\n",
    "\n",
    "            new__fights_data = pd.read_csv(self.NEW_FIGHTS_PATH)\n",
    "            old_fights_data = pd.read_csv(self.TOTAL_FIGHTS_PATH)\n",
    "\n",
    "            # verify same column count\n",
    "            assert len(new_fights_data.columns) == len(\n",
    "                old_fights_data.columns\n",
    "            )\n",
    "\n",
    "            # restricts new event cols to those with labels of old events/ensures same col order\n",
    "            # feels like merging new/old fight data should be a seperate method\n",
    "            new_fights_data = new_fights_data[list(old_fights_data.columns)]\n",
    "\n",
    "            # might be worth verifying integrity here\n",
    "            latest_total_fight_data = pd.concat(\n",
    "                [new_fights_data, old_fights_data],\n",
    "                axis=1,\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "            latest_total_fight_data.to_csv(self.TOTAL_FIGHTS_PATH, index=None)\n",
    "            print(f\"Updated {self.TOTAL_FIGHTS_PATH} with new fight data\")\n",
    "            os.remove(self.NEW_EVENT_AND_FIGHTS_PATH)\n",
    "            print(\"Removed temporary files.\")\n",
    "\n",
    "        print(\"Successfully scraped and saved UFC fight data!\")\n",
    "\n",
    "    def _scrape_raw_fight_data(\n",
    "        self, event_and_fight_links: Dict[str, List[str]], filepath\n",
    "    ):\n",
    "        if filepath.exists():\n",
    "            print(f\"File {filepath} already exists, overwriting.\")\n",
    "\n",
    "        total_stats = self._get_total_fight_stats(event_and_fight_links)\n",
    "        with open(filepath.as_posix(), \"wb\") as file:\n",
    "            file.write(bytes(self.HEADER, encoding=\"ascii\", errors=\"ignore\"))\n",
    "            file.write(bytes(total_stats, encoding=\"ascii\", errors=\"ignore\"))\n",
    "\n",
    "    def _get_fight_stats_task(self, fight, event_info):\n",
    "        total_fight_stats = \"\"\n",
    "        try:\n",
    "            fight_soup = make_soup(fight)\n",
    "            fight_stats = self._get_fight_stats(fight_soup)\n",
    "            fight_details = self._get_fight_details(fight_soup)\n",
    "            result_data = self._get_fight_result_data(fight_soup)\n",
    "            total_fight_stats = (\n",
    "                fight_stats + \";\" + fight_details + \";\" + event_info + \";\" + result_data\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Error getting fight stats, \" + str(e))\n",
    "            pass\n",
    "\n",
    "        return total_fight_stats\n",
    "\n",
    "    def _get_total_fight_stats(self, fight_links: Dict[str, List[str]]) -> str:\n",
    "        total_stats = \"\"\n",
    "\n",
    "        fight_count = len(fight_links)\n",
    "        print(f\"Scraping data for {fight_count} fights: \")\n",
    "        print_progress(0, fight_count, prefix=\"Progress:\", suffix=\"Complete\")\n",
    "\n",
    "        for index, (event, fights) in enumerate(fight_links.items()):\n",
    "            event_soup = make_soup(event)\n",
    "            event_info = self._get_event_info(event_soup)\n",
    "\n",
    "            # Get data for each fight in the event in parallel.\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                futures = []\n",
    "                for fight in fights:\n",
    "                    futures.append(\n",
    "                        executor.submit(\n",
    "                            self._get_fight_stats_task,\n",
    "                            fight=fight,\n",
    "                            event_info=event_info,\n",
    "                        )\n",
    "                    )\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    fight_stats = future.result()\n",
    "                    if fight_stats != \"\":\n",
    "                        if total_stats == \"\":\n",
    "                            total_stats = fight_stats\n",
    "                        else:\n",
    "                            total_stats = total_stats + \"\\n\" + fight_stats\n",
    "                    print_progress(index + 1, fight_count, prefix=\"Progress:\", suffix=\"Complete\")\n",
    "\n",
    "        return total_stats\n",
    "\n",
    "    def _get_fight_stats(self, fight_soup: BeautifulSoup) -> str:\n",
    "        tables = fight_soup.findAll(\"tbody\")\n",
    "        # hard coded to grab totals and significant strike stats.\n",
    "        # skips per round stats\n",
    "        # i think we want per round stats.\n",
    "        total_fight_data = [tables[0], tables[2]]\n",
    "        fight_stats = []\n",
    "        for table in total_fight_data:\n",
    "            row = table.find(\"tr\")\n",
    "            stats = \"\"\n",
    "            for data in row.findAll(\"td\"):\n",
    "                if stats == \"\":\n",
    "                    stats = data.text\n",
    "                else:\n",
    "                    stats = stats + \",\" + data.text\n",
    "            fight_stats.append(\n",
    "                stats.replace(\"  \", \"\")\n",
    "                .replace(\"\\n\\n\", \"\")\n",
    "                .replace(\"\\n\", \",\")\n",
    "                .replace(\", \", \",\")\n",
    "                .replace(\" ,\", \",\")\n",
    "            )\n",
    "\n",
    "        #hardcoded here to ignore first 3 cols of significant strikes table\n",
    "        fight_stats[1] = \";\".join(fight_stats[1].split(\",\")[6:])\n",
    "        fight_stats[0] = \";\".join(fight_stats[0].split(\",\"))\n",
    "        fight_stats = \";\".join(fight_stats)\n",
    "        return fight_stats\n",
    "\n",
    "    def _get_fight_details(self, fight_soup: BeautifulSoup) -> str:\n",
    "        columns = \"\"\n",
    "        for div in fight_soup.findAll(\"div\", {\"class\": \"b-fight-details__content\"}):\n",
    "            for col in div.findAll(\"p\", {\"class\": \"b-fight-details__text\"}):\n",
    "                if columns == \"\":\n",
    "                    columns = col.text\n",
    "                else:\n",
    "                    columns = columns + \",\" + (col.text)\n",
    "\n",
    "        columns = (\n",
    "            columns.replace(\"  \", \"\")\n",
    "            .replace(\"\\n\\n\\n\\n\", \",\")\n",
    "            .replace(\"\\n\", \"\")\n",
    "            .replace(\", \", \",\")\n",
    "            .replace(\" ,\", \",\")\n",
    "            .replace(\"Method: \", \"\")\n",
    "            .replace(\"Round:\", \"\")\n",
    "            .replace(\"Time:\", \"\")\n",
    "            .replace(\"Time format:\", \"\")\n",
    "            .replace(\"Referee:\", \"\")\n",
    "        )\n",
    "\n",
    "        fight_details = \";\".join(columns.split(\",\")[:5])\n",
    "\n",
    "        return fight_details\n",
    "\n",
    "\n",
    "    def _get_event_info(self, event_link: str) -> str:\n",
    "        # use hash in URL as event id.\n",
    "        event_id = event_link.split('/')[-1]\n",
    "\n",
    "        event_soup = make_soup(event_link)\n",
    "        event_title = event_soup.find('h2', {\"class\":\"b-content__title\"}).text.strip()\n",
    "        # take whatever's after the colon, strip whitespace and upper case it.\n",
    "        # hoping it's just date/location respectively, otherwise this is gonna get wonky.\n",
    "        event_attr = [attr.text.split(':')[-1].strip().upper() for attr in event_soup.findAll(\"li\", {\"class\": \"b-list__box-list-item\"})]\n",
    "\n",
    "        # should spit out semicolon seperated string\n",
    "        # id;title;date;location\n",
    "        event_info =\";\".join([event_id, event_title] + event_attr)\n",
    "\n",
    "        return event_info\n",
    "\n",
    "    def _get_fight_result_data(self, fight_soup: BeautifulSoup) -> str:\n",
    "        winner = \"\"\n",
    "        for div in fight_soup.findAll(\"div\", {\"class\": \"b-fight-details__person\"}):\n",
    "            if (\n",
    "                div.find(\n",
    "                    \"i\",\n",
    "                    {\n",
    "                        \"class\": \"b-fight-details__person-status b-fight-details__person-status_style_green\"\n",
    "                    },\n",
    "                )\n",
    "                is not None\n",
    "            ):\n",
    "                winner = (\n",
    "                    div.find(\"h3\", {\"class\": \"b-fight-details__person-name\"})\n",
    "                    .text.replace(\" \\n\", \"\")\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                )\n",
    "\n",
    "        fight_type = (\n",
    "            fight_soup.find(\"i\", {\"class\": \"b-fight-details__fight-title\"})\n",
    "            .text.replace(\"  \", \"\")\n",
    "            .replace(\"\\n\", \"\")\n",
    "        )\n",
    "\n",
    "        return fight_type + \";\" + winner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing fighter names/results here\n",
    "\n",
    "# given single \"b-fight-details__person\" element, get name, link and result.\n",
    "def _get_fighter(fighter_raw: BeautifulSoup) -> dict:\n",
    "    name = fighter_raw.a.text.strip().upper()\n",
    "    link = fighter_raw.a.get('href')\n",
    "    id = link.split(\"/\")[-1]\n",
    "    result = fighter_raw.i.text.strip().upper()\n",
    "\n",
    "    fighter = {\"FIGHTER\": name,\n",
    "               \"FIGHTER_ID\": id,\n",
    "               \"FIGHTER_LINK\":link,\n",
    "               \"FIGHTER_RESULT\": result}\n",
    "\n",
    "\n",
    "    return fighter\n",
    "\n",
    "def _get_fighters(fight_soup: BeautifulSoup) -> dict:\n",
    "    fighters={}\n",
    "\n",
    "    r_raw, b_raw = fight_soup.find_all('div', {\"class\": \"b-fight-details__person\"})\n",
    "\n",
    "    r = _get_fighter(r_raw)\n",
    "    b = _get_fighter(b_raw)\n",
    "\n",
    "    fighters = add_prefix_label(r, \"R\") | add_prefix_label(b, \"B\")\n",
    "\n",
    "    return fighters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping fight attributes (everything in the the non-tabular box) and all associated routines HERE\n",
    "\n",
    "# fight name might say HEAVYWEIGHT BOUT, or UFC TITLE HEAVYWEIGHT BOUT\n",
    "# helper function picks out the word with 'weight' in it\n",
    "# expects string to already be stripped/capitalized/seperated by spaces\n",
    "def _parse_weightclass(fight_name):\n",
    "    for word in fight_name.split(\" \"):\n",
    "        if \"WEIGHT\" in word:\n",
    "            return word\n",
    "        else:\n",
    "            continue\n",
    "    # no word with \"weight\" in fight name\n",
    "    return \"WEIGHTCLASS PARSING ERROR\"\n",
    "\n",
    "# couple of cases here because of changes in UFC methodology that i'm merging together.\n",
    "# current UFC awards FOTN and performance bonuses for best finishes\n",
    "# used to award KOTN and SOTN specficially and icons indicating these are still\n",
    "# in data. i'm gonna record them all as performance bonuses.\n",
    "# FOTN still distinguished seperately.\n",
    "# all of these are are just marked by embedded images so this check is super hardcoded.\n",
    "def _is_perf_bonus(attr_soup: BeautifulSoup) -> bool:\n",
    "    for img in attr_soup.i.find_all(\"img\"):\n",
    "        src = img.get('src')\n",
    "        if (\"ko.png\" in src) or (\"perf.png\" in src) or (\"fight.png\" in src) or (\"sub.png\" in src):\n",
    "            return True\n",
    "    # otherwise false\n",
    "    return False\n",
    "\n",
    "    # this function might be useful other places, might generalize\n",
    "def _parse_attr(p_soup:BeautifulSoup) -> dict:\n",
    "    # each top level i tag in this p block is one attr\n",
    "    attr_dict = {}\n",
    "    for i_raw in p_soup.findAll('i', recursive=False):\n",
    "        # smash (like khamzat) together, then split at :\n",
    "        attr= \" \".join(i_raw.stripped_strings).upper().split(': ')\n",
    "        attr_lbl = attr[0]\n",
    "        attr_txt = attr[1]\n",
    "        # print(attr)\n",
    "        # print(attr_lbl)\n",
    "        # print(attr_txt)\n",
    "        attr_dict[attr_lbl] = attr_txt\n",
    "\n",
    "\n",
    "    return attr_dict\n",
    "\n",
    "# given \"b-fight-details__fight\" html soup,\n",
    "# parses content (\"b-fight-details__content\") (method, round, etc)\n",
    "def _get_attr_content(attr_soup: BeautifulSoup) -> Dict:\n",
    "    # logic for parsing attributes out of the two p blocks\n",
    "    # is annoying because of inconsistency in i tag usage.\n",
    "\n",
    "    # ASSUMING THAT THE FOLLOWING CALL ONLY FINDS TWO P TAGS (one with method/round/etc. and second with details:)\n",
    "    # each has its own parsing\n",
    "    p_attr, p_details = attr_soup.find('div', {\"class\": \"b-fight-details__content\"}).findAll('p')\n",
    "\n",
    "    attr_dict = _parse_attr(p_attr)\n",
    "\n",
    "    #details value is special case\n",
    "    attr_dict[\"DETAILS\"]= \" \".join(p_details.stripped_strings).upper().split(\": \")[-1]\n",
    "\n",
    "    return attr_dict\n",
    "\n",
    "\n",
    "\n",
    "def _get_fight_attr(fight_soup: BeautifulSoup) -> Dict:\n",
    "    attr_raw = fight_soup.find('div', {\"class\": \"b-fight-details__fight\"})\n",
    "    fight_name = attr_raw.i.text.strip().upper()\n",
    "    weight = _parse_weightclass(fight_name)\n",
    "\n",
    "    # detecting title fights by the word \"TITLE\" in fight name\n",
    "    # could also do this by looking for belt icon/css tag\n",
    "    title_fight = \"TITLE\" in fight_name\n",
    "    perf_bonus = _is_perf_bonus(attr_raw)\n",
    "\n",
    "    # initialize attr_dict with attr content then manually add\n",
    "    # weight class, title fight and performance bonus flags\n",
    "    attr_dict = _get_attr_content(attr_raw)\n",
    "    attr_dict[\"WEIGHT_CLASS\"] = weight\n",
    "    attr_dict[\"TITLE_FIGHT\"] = title_fight\n",
    "    attr_dict[\"PERF_BONUS\"] = perf_bonus\n",
    "\n",
    "    return attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fight_stats(fight_link: str) -> dict:\n",
    "    fight_soup = make_soup(fight_link)\n",
    "\n",
    "    # - 4 things to grab\n",
    "    # - fighter details (name, result, fighter_link (or ID))\n",
    "    # - fight attributes (winner, method, etc -- everything before the tables)\n",
    "    # - general stats (kd, td, sub att, rev,  ctrl) - these are deceptively labeled TOTALS\n",
    "    # (we don't need sig str cols from this table b/c they're repeated in the strike table )\n",
    "    # - sig strike data.\n",
    "\n",
    "    #initiate fight_stats dict with LINK and ID.\n",
    "    fight_id = fight_link.split(\"/\")[-1]\n",
    "    fight_stats={\"FIGHT_ID\": fight_id,\n",
    "                 \"FIGHT_LINK\": fight_link}\n",
    "\n",
    "    fight_fighters = _get_fighters(fight_soup)\n",
    "    fight_attr = _get_fight_attr(fight_soup)\n",
    "    ###########\n",
    "\n",
    "    # EVERYTHING BELOW THIS IS TO DO\n",
    "    fight_gen_stats = _get_fight_gen_stats(fight_soup)\n",
    "    fight_strike_stats = _get_fight_strike_stats(fight_soup)\n",
    "\n",
    "    fight_stats.update(fight_attr)\n",
    "    fight_stats.update(fight_gen_stats)\n",
    "    fight_stats.update(fight_strike_stats)\n",
    "\n",
    "    return fight_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_soup = make_soup('http://ufcstats.com/event-details/39f68882def7a507')\n",
    "volk_lopes_soup = make_soup('http://ufcstats.com/fight-details/e733f148060bef2a')\n",
    "krylov_reyes_soup = make_soup('http://ufcstats.com/fight-details/b2d731415bd367df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
